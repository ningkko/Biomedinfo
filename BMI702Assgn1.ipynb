{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BMI702Assgn1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ningkko/Biomedinfo/blob/master/BMI702Assgn1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authored by Yasha Ektefaie and Mert Eden"
      ],
      "metadata": {
        "id": "XgblYbx6kSBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not change the runtime environment from GPU to standard runtime. You'll need GPU for Q3. **"
      ],
      "metadata": {
        "id": "ayXK6Hstkqq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please make a copy of this Colab notebook so that you can save your codes!***"
      ],
      "metadata": {
        "id": "pu9y-NOLkvLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fairness:\n",
        "\n",
        "Code blocks titled 2.1 and 2.2 pertain to questions 2.1 and 2.2 respectively on the homework. Below are some blocks to set you up for training the logistic regression model. It will download the dataset and do some preproccessing. For this exercise *you will not be training the model yourself*. "
      ],
      "metadata": {
        "id": "uHzMd2j1kzW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xi-y4FPzkV8E",
        "outputId": "8ee6c7ac-6a51-42c8-e3b2-6491cb61be43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-14 00:57:15--  https://www.openml.org/data/get_csv/31/dataset_31_credit-g.csv\n",
            "Resolving www.openml.org (www.openml.org)... 131.155.11.11\n",
            "Connecting to www.openml.org (www.openml.org)|131.155.11.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘credit.csv’\n",
            "\n",
            "\rcredit.csv              [<=>                 ]       0  --.-KB/s               \rcredit.csv              [ <=>                ] 148.44K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-03-14 00:57:15 (5.20 MB/s) - ‘credit.csv’ saved [152006]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O credit.csv https://www.openml.org/data/get_csv/31/dataset_31_credit-g.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "RIV5ZpXykgzV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('credit.csv')"
      ],
      "metadata": {
        "id": "AwpJlTKUkkVo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove = ['installment_commitment', 'other_parties','housing', 'property_magnitude',\n",
        "          'other_payment_plans','existing_credits', 'residence_since']\n",
        "  \n",
        "cols_keep = [i for i in df.columns if i not in remove]"
      ],
      "metadata": {
        "id": "xKdFfPpUkmUi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking status different levels of credit\n",
        "checking_status_mappings = {\"'no checking'\": 0, \"'<0'\": 1, \"'0<=X<200'\":2, \"'>=200'\":3}\n",
        "\n",
        "#credit problem: have they paid (0) if not paid then problem 1\n",
        "credit_status_mappings = {\"'no credits/all paid'\":0, \"'all paid'\":0,\n",
        "                          \"'existing paid'\":0,\n",
        "                          \"'critical/other existing credit'\":1,\n",
        "                          \"'delayed previously'\":1 }\n",
        "\n",
        "#credit pupose: car is 0, education is 1, business is 2, objects: 3, other: 4\n",
        "credit_purpose_mapping = {\n",
        "    \"radio/tv\": 3,\n",
        "    \"education\": 1,\n",
        "    \"furniture/equipment\":3,\n",
        "    \"'new car'\": 0 ,\n",
        "    \"'used car'\":0, \n",
        "    \"business\": 2, \n",
        "    \"'domestic appliance'\": 3,\n",
        "    \"repairs\": 3,\n",
        "    \"other\": 4, \n",
        "    \"retraining\": 4\n",
        "    }\n",
        "\n",
        "#employment: unemployed is 0, <1 is 1, 1<=X<4 is 2, 4<=X<7 is 3, >= 7 is 4\n",
        "employment_mapping = {\n",
        "    \"'>=7'\": 4, \n",
        "    \"'1<=X<4'\": 2, \n",
        "    \"'4<=X<7'\": 3, \n",
        "    'unemployed': 0 , \n",
        "    \"'<1'\": 1\n",
        "}\n",
        "\n",
        "#saving status: no savings is 0, <100 is 1, 100-500 is 2, 500-1000 is 3, >1000 4\n",
        "savings_status = {\"'no known savings'\":0, \n",
        "\"'<100'\":1, \n",
        "\"'500<=X<1000'\":3, \n",
        "\"'>=1000'\":4,\n",
        "\"'100<=X<500'\": 2}\n",
        "\n",
        "#job binning just provide number to each option\n",
        "job_mapping = {\n",
        "    'skilled': 0, \n",
        "    \"'unskilled resident'\": 1, \n",
        "    \"'high qualif/self emp/mgmt'\": 2,\n",
        "    \"'unemp/unskilled non res'\": 3}"
      ],
      "metadata": {
        "id": "_Ugnz8-Hr2lx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trunc_df = df[cols_keep].copy()\n",
        "trunc_df = trunc_df.replace({\n",
        "                  \"checking_status\": checking_status_mappings,\n",
        "                  \"purpose\": credit_purpose_mapping, \n",
        "                  \"credit_history\": credit_status_mappings,\n",
        "                  \"employment\": employment_mapping,\n",
        "                  \"job\": job_mapping,\n",
        "                  \"savings_status\":savings_status})\n",
        "\n",
        "#Gender 1 if female, 0 if male\n",
        "trunc_df['sex'] = np.where(trunc_df['personal_status'].str.contains('female'), 1, 0)\n",
        "#Telephone 1 if have telephone, 0 otherwise\n",
        "trunc_df['own_telephone'] = np.where(trunc_df['own_telephone'].str.contains('none'), 0, 1)\n",
        "#Foreign worker 1 if foreign worker, 0 otherwise\n",
        "trunc_df['foreign_worker'] = np.where(trunc_df['foreign_worker'].str.contains('yes'), 1, 0)\n",
        "#Class 1 if good, 0 if bad\n",
        "trunc_df['class'] = np.where(trunc_df['class'].str.contains('good'), 1, 0)\n",
        "del trunc_df['personal_status']\n",
        "trunc_df.head()"
      ],
      "metadata": {
        "id": "w0wrHBErse3y",
        "outputId": "17a9d425-4847-419a-a5e4-8d758b353f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   checking_status  duration  credit_history  purpose  credit_amount  \\\n",
              "0                1         6               1        3           1169   \n",
              "1                2        48               0        3           5951   \n",
              "2                0        12               1        1           2096   \n",
              "3                1        42               0        3           7882   \n",
              "4                1        24               1        0           4870   \n",
              "\n",
              "   savings_status  employment  age  job  num_dependents  own_telephone  \\\n",
              "0               0           4   67    0               1              1   \n",
              "1               1           2   22    0               1              0   \n",
              "2               1           3   49    1               2              0   \n",
              "3               1           3   45    0               2              0   \n",
              "4               1           2   53    0               2              0   \n",
              "\n",
              "   foreign_worker  class  sex  \n",
              "0               1      1    0  \n",
              "1               1      0    1  \n",
              "2               1      1    0  \n",
              "3               1      1    0  \n",
              "4               1      0    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b36052f-6f3b-45aa-be09-ac65c54d02e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checking_status</th>\n",
              "      <th>duration</th>\n",
              "      <th>credit_history</th>\n",
              "      <th>purpose</th>\n",
              "      <th>credit_amount</th>\n",
              "      <th>savings_status</th>\n",
              "      <th>employment</th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>num_dependents</th>\n",
              "      <th>own_telephone</th>\n",
              "      <th>foreign_worker</th>\n",
              "      <th>class</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1169</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5951</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2096</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7882</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4870</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b36052f-6f3b-45aa-be09-ac65c54d02e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b36052f-6f3b-45aa-be09-ac65c54d02e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b36052f-6f3b-45aa-be09-ac65c54d02e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, x_valid, y_train, y_valid = train_test_split(trunc_df.drop('class',axis=1), \n",
        "                                                    trunc_df['class'], test_size=0.30, \n",
        "                                                    random_state=101)\n"
      ],
      "metadata": {
        "id": "jrUauKtasvVt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biased_lr = LogisticRegression(max_iter = 1000)\n",
        "biased_lr.fit(X_train,y_train)\n",
        "print(f\"Logistic regression valid accuracy: {biased_lr.score(x_valid, y_valid)}\")"
      ],
      "metadata": {
        "id": "ALdt2Pw0ZSm6",
        "outputId": "4ac056ec-7e2b-42c7-ecdd-92b16e39591e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression valid accuracy: 0.7033333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Compute the demographic parity with respect to the sensitive trait `sex`.\n",
        "\n",
        "Define a function parity that takes the predictions on the test set of our\n",
        "linear regressor, and a column of sensitive attributes and computes the difference in demographic parity. \n",
        "\n",
        "Run parity on the sensitive attribute `sex` and briefly discuss whether or not this linear regressor satisfies demographic parity."
      ],
      "metadata": {
        "id": "65q7KQa-bBU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are comparing the rates of good credit among males versus females\n",
        "\n",
        "def parity(pred, sens):\n",
        "  return 0\n",
        "\n",
        "sens = x_valid['sex']\n",
        "pred = biased_lr.predict(x_valid)\n",
        "\n",
        "parity(pred, sens)"
      ],
      "metadata": {
        "id": "7zytyBIWaDhB",
        "outputId": "d8d3410d-e4f0-4c2d-db19-c1aa9a400cab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Compute equal opportunity with respect to the sensitive trait `sex`.\n",
        "Define a function equal_opportunity that takes the predictions on the test set of our linear regressor, a column of ground truth labels, and a column of sensitive attributes and computes the difference in equal opportunity.\n",
        "\n",
        "Compute equal opportunity with respect to the sensitive trait `sex`. How can we interpret the calculated rate of equal opportunity? What is the meaning of this rate for someone looking to get a loan from the bank?"
      ],
      "metadata": {
        "id": "XLbPyYxUlooA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_opportunity(pred, labels, sens):\n",
        "  return 0\n",
        "sens = x_valid['sex']\n",
        "labels = y_valid\n",
        "pred = biased_lr.predict(x_valid)\n",
        "equal_opportunity(pred, labels, sens)"
      ],
      "metadata": {
        "id": "aqcrEwJwfMzP",
        "outputId": "727a3613-8756-4a73-bee2-e5c4bf418355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Explainibility.\n",
        "\n",
        "**Before you start working on Q3, in case you have changed your runtime, check if you still have GPU resource by clicking on Runtime > Change runtime type. \"Hardware acceleartor\" should be set as \"GPU\". **\n",
        "\n",
        "For the next problem you will be running integrated gradients on a binary classifier trained on the PCam dataset. PCam or PatchCamelyon is a histopathological dataset of lymph nodes. The classifier is trained to detect whether or not there is any metastatis present in these lymph nodes. Below is some starter code to download required packages, install the dataset and train the model. Code blocks are titled with the respective excerise number in the assignment."
      ],
      "metadata": {
        "id": "96Hf2CQcl6tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "We'll be using pyTorch to train our CNN. **All the NN training is already implemented so you do not have to understand how it works.** Please just change the runtime type as explained above and run the following lines of code one at a time. "
      ],
      "metadata": {
        "id": "Ha2AmUrPl9V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting pytorch\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install captum\n",
        "!pip install flask_compress\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "sHljojTGnsxi",
        "outputId": "2d01499a-7f82-438a-af2b-6cf31a7b00e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.3 MB 25 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.0\n",
            "  Downloading torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (1.21.5)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1+cu101) (7.1.2)\n",
            "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0+cu101 torchaudio-0.7.0 torchvision-0.8.1+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 163 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 266 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 307 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 317 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 327 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 337 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 348 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 450 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 460 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 471 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 481 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 501 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 512 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 522 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 532 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 604 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 614 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 634 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 645 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 655 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 665 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 675 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 686 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 696 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 706 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 768 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 778 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 788 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 798 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 808 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 819 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 829 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 839 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 860 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 870 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 880 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 890 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 901 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 911 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 921 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 931 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 942 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 952 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 962 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 972 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 993 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.0 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.21.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from captum) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->captum) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->captum) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->captum) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.5.0\n",
            "Collecting flask_compress\n",
            "  Downloading Flask_Compress-1.11-py3-none-any.whl (7.9 kB)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from flask_compress) (1.1.4)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->flask_compress) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->flask_compress) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->flask_compress) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->flask_compress) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->flask_compress) (2.0.1)\n",
            "Installing collected packages: brotli, flask-compress\n",
            "Successfully installed brotli-1.0.9 flask-compress-1.11\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=7d58a75e12998cc613b08f076e98ec40adaa2bc1827abbfdb28afb0cd4796b62\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hjq0agbi/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2\n",
        "!gdown --id 1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG"
      ],
      "metadata": {
        "id": "0artcDfGmPKQ",
        "outputId": "9a67d247-bca4-4730-f15c-ab2a5ca8ba4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2\n",
            "To: /content/camelyonpatch_level_2_split_train_x.h5.gz\n",
            "100% 6.42G/6.42G [01:11<00:00, 89.7MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG\n",
            "To: /content/camelyonpatch_level_2_split_train_y.h5.gz\n",
            "100% 21.4k/21.4k [00:00<00:00, 27.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d camelyonpatch_level_2_split_train_x.h5.gz\n",
        "!gzip -d /content/camelyonpatch_level_2_split_train_y.h5.gz"
      ],
      "metadata": {
        "id": "VYk_OyfB0YPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jH4OPAOFnjFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/gdrive/My\\ Drive/BMI702/ # You can change directory under 'My Drive' as you want\n",
        "!cp camelyonpatch_level_2_split_train_x.h5 /content/gdrive/My\\ Drive/BMI702/"
      ],
      "metadata": {
        "id": "jUjCYDct0Gfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Setup Dataset"
      ],
      "metadata": {
        "id": "QpfRTsYJJ8r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PER_TRAIN = 50 / 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch import optim\n",
        "from torch.autograd import Variable \n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "import h5py \n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class PCam(torch.utils.data.Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "      super(PCam, self).__init__()\n",
        "      self.images = h5py.File('/content/camelyonpatch_level_2_split_train_x.h5', 'r')\n",
        "      self.labels = h5py.File('/content/camelyonpatch_level_2_split_train_y.h5', 'r')\n",
        "      self.transform=transform\n",
        "\n",
        "  def __getitem__(self, index): \n",
        "    image = torch.tensor(self.images['x'][index]).float()\n",
        "    image = (image / 255.0).permute(2,1,0)\n",
        "    if self.transform:\n",
        "      image = self.transform(image, self.labels['y'][index])\n",
        "\n",
        "    return (image, Variable(torch.tensor(self.labels['y'][index]).reshape(-1).float()))\n",
        "  def __len__(self):\n",
        "    return len(self.images['x'])\n",
        "\n"
      ],
      "metadata": {
        "id": "kQ66c4EU0NCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_transform(tensor, label):\n",
        "  transform = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ToPILImage(), # Convert np array to PILImage\n",
        "      \n",
        "      # Resize image to 224 x 224 as required by most vision models\n",
        "      torchvision.transforms.Resize(\n",
        "          size=(224, 224)\n",
        "      ),\n",
        "      \n",
        "      # Convert PIL image to tensor with image values in [0, 1]\n",
        "      torchvision.transforms.ToTensor(),\n",
        "  ])\n",
        "  return transform(tensor)\n",
        "\n",
        "\n",
        "pcam = PCam(transform=initial_transform)\n",
        "\n",
        "sz = len(pcam)\n",
        "size_train = math.floor(sz - (sz * (1 - PER_TRAIN)))\n",
        "size_test = sz - size_train\n",
        "\n",
        "print(f\"Training size: {size_train}\")\n",
        "print(f\"Testing size: {size_test}\")\n",
        "\n",
        "\n",
        "train_split, test_split = torch.utils.data.random_split(pcam, [size_train, size_test])\n",
        "train_set = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, num_workers=1)\n",
        "test_set = DataLoader(test_split, batch_size=1,\n",
        "                        shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "NdLIOoQ02CEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a peek of our data\n",
        "Our transformed PCAM dataset consists of 262,144 data each consists of a 224*224 image with 3 color channels and 1 label to annotate if the image contains metastatic tissue (1) or not (0)."
      ],
      "metadata": {
        "id": "484B98Gx2JsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = pcam[0]\n",
        "print(\"Image of shape \", image.shape)\n",
        "print(image)\n",
        "print(\"Label: \", label)\n",
        "\n",
        "\"\"\"\n",
        "If everything ran correctly should see:Image of shape  torch.Size([3, 224, 224])\n",
        "And then should see a large printed out 2D Array with text like:\n",
        "\n",
        "tensor([[[0.8863, 0.8863, 0.8784,  ..., 0.3451, 0.3373, 0.3333],\n",
        "\n",
        "printed out.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DkRf4Ywx2Fre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "*********************\n",
        "*********************\n",
        "!!!!!PLEASE READ!!!!!\n",
        "*********************\n",
        "*********************\n",
        "\n",
        "This is the code to train the model. \n",
        "If everything above ran correctly upon running this block you should see\n",
        "lines like: \n",
        "\n",
        "[1,     1] loss: 0.00021\n",
        "Accuracy: 0.010625000111758709\n",
        "\n",
        "Make sure you switched the runtime to GPU as described in the beginning.\n",
        "Even with the GPU this block of code will take around 15-30 minutes to run. \n",
        "As long as lines like the one shown above are being printed and the accuracy\n",
        "is increasing everything is fine. Just don't loose internet connect or let your\n",
        "computer fall asleep during training. \n",
        "\n",
        "It is important you run this block of code once. Do not rerun it as you will\n",
        "rerun training. This is why after this block of code there is code to SAVE\n",
        "your model and LOAD the model. That way you can run training and save the model\n",
        "once and then load as needed.\n",
        "\n",
        "The way all this works is we explicitly create a folder in your google drive \n",
        "where we are storing the training data and saved model. Once the training data\n",
        "and model is saved, even if your runtime disconnects or you close this tab\n",
        "the model and data is still saved to your google drive. All you have to do \n",
        "to start up again is remount to that directory and load in the model. \n",
        "\n",
        "Run these next cells carefully.\n",
        "\n",
        "\"\"\"\n",
        "import torchvision.models as models\n",
        "from tqdm.notebook import tqdm\n",
        "import gc \n",
        "import copy \n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "MY_MODEL = models.resnet18() \n",
        "\n",
        "\"\"\"\n",
        "If you find training is taking too long consider changing epoch here from 2 to 1\n",
        "So instead of epochs = 2, have it be epochs = 1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def train(dataset, epochs=2):\n",
        "  model = copy.deepcopy(MY_MODEL)\n",
        "\n",
        "  model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 1))\n",
        "\n",
        "  error = nn.BCEWithLogitsLoss()\n",
        "  learning_rate = 0.001\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  running_loss = 0.0\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model.train()\n",
        "  model = model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    for i, (train, labels) in enumerate(dataset):\n",
        "            # Clear gradients\n",
        "            train = train.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if i == 0:\n",
        "              print(\"here\")\n",
        "            # Forward propagation\n",
        "            outputs = model(train)        \n",
        "            # Compute Loss\n",
        "            loss = error(outputs, labels.view(-1, 1).float())\n",
        "            correct += labels.eq(torch.round(torch.sigmoid(outputs))).sum()\n",
        "            # Calculating gradients\n",
        "            loss.backward()        \n",
        "            # Update parameters\n",
        "            optimizer.step()   \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 50 == 0:    # print every 50 mini-batches\n",
        "                print('[%d, %5d] loss: %.5f' %\n",
        "                      (epoch + 1, i + 1, running_loss / (BATCH_SIZE * 50)))\n",
        "                running_loss = 0.0\n",
        "                print(f\"Accuracy: {correct / (BATCH_SIZE * 50)}\")\n",
        "                correct = 0\n",
        "  return model\n",
        "\n",
        "#model = train(train_set)"
      ],
      "metadata": {
        "id": "mXk_zn9G2MfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the code to train the model\n",
        "model = train(train_set)"
      ],
      "metadata": {
        "id": "uzpPsgeuRwyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can save the model to your drive and load it in the future."
      ],
      "metadata": {
        "id": "CZLLABipBW3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/gdrive/My\\ Drive/BMI702/ # You can change the directory under \"My Drive\" as you want."
      ],
      "metadata": {
        "id": "rVQOG_ZQ5JSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to save your model\n",
        "model_save_name = 'pcam_model'\n",
        "path = f\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "9lxcQfsqBZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to load the model\n",
        "#If your collab notebook instance crashes you can reload the model by running these lines\n",
        "#We first remount to the google drive folder were everything is stored\n",
        "#Then we just pull from the model file stored in that folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'pcam_model'\n",
        "path = f\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "model = torch.load(path)"
      ],
      "metadata": {
        "id": "ihcfSWZHBc-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate how our model did on the first 2000 testing points"
      ],
      "metadata": {
        "id": "YF379bU_BgZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, (train, labels) in enumerate(test_set):\n",
        "      if i > 2000:\n",
        "        break\n",
        "      images = train.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      correct += labels.view(-1,1).eq(torch.round(torch.sigmoid(outputs))).sum()\n",
        "      total += 1\n",
        "      \n",
        "\n",
        "print('Accuracy of the network on the 2000 test images: %d %%' % (100 * (correct / total)))"
      ],
      "metadata": {
        "id": "5RP8FTSvBfOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's visualize some slides.\n",
        "\n",
        "Although we're no pathologists, we do get some valuable insight as to what our data is by looking at it. Below is some simple matplotlib code to get you started."
      ],
      "metadata": {
        "id": "s7Cf2uM2Cacl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import visualization as viz\n",
        "from captum.insights import AttributionVisualizer, Batch\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-kl40r4JCK_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(5,2, figsize=(20,20))\n",
        "\n",
        "for i, (img, label) in enumerate(test_set):\n",
        "  if i >= 10:\n",
        "    break\n",
        "  else:\n",
        "    img = img.squeeze(dim=0)\n",
        "    ax[i//2,i%2].title.set_text(\"Metastases\" if label == 1 else \"Benign\")\n",
        "    ax[i//2,i%2].imshow(img.permute(1,2,0))"
      ],
      "metadata": {
        "id": "14vepZ40bMva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great! Now you have trained and saved a model. Now we can begin answering some questions! This is where you will have to do some coding."
      ],
      "metadata": {
        "id": "E3j-UVj3Cw_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 How explainable are our results? \n",
        "Run the code below to pick an image from 'test_set' and run integrated gradients. Use the output to explain whether or not the model is really looking for characteristics of metastasis. Report in your results whether or not the integrated gradients tell a meaningful story about the image. We do not expect a medical answer here, any form of reasoning is fine."
      ],
      "metadata": {
        "id": "N7ei8hRvChqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If you run this and get a GPU out of memory error. Restart the kernal and \n",
        "reload the data and reload your saved model. The training caused the GPU\n",
        "memory to get filled up.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def visualize_ig(model, img, label, baseline):\n",
        "  img = img.unsqueeze(0).to(device)\n",
        "  outputs = model(img)\n",
        "  print(f\"Raw model Output {outputs.item()}\")\n",
        "  outputs = nn.functional.softmax(outputs).reshape(-1)\n",
        "  print(f\"Model output after running through softmax {outputs.item()}\")\n",
        "  integrated_gradients = IntegratedGradients(model)\n",
        "\n",
        "  attributions_ig = integrated_gradients.attribute(img, target=0)\n",
        "\n",
        "  default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
        "                                                  [(0, '#ffffff'),\n",
        "                                                  (0.25, '#000000'),\n",
        "                                                  (1, '#000000')], N=256)\n",
        "  noise_tunnel = NoiseTunnel(integrated_gradients)\n",
        "  target = int(label)\n",
        "  attributions_ig_nt = noise_tunnel.attribute(img, nt_samples=10, nt_type='smoothgrad_sq')\n",
        "  _ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      [\"original_image\", \"heat_map\"],\n",
        "                                      [\"all\", \"positive\"],\n",
        "                                      cmap=default_cmap,\n",
        "                                        show_colorbar=True)\n"
      ],
      "metadata": {
        "id": "WPuYzTrcCgWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)"
      ],
      "metadata": {
        "id": "8gWjPBDJao7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 How does occlusion effect our results?\n",
        "\n",
        "Write a function `occlude` that takes in a image and randomly places a grey square that obstructs a part of the image. Then run the code to re-run the classifier on the occluded image comparing the model and integrated gradient output of the occluded verses the non-occluded image. "
      ],
      "metadata": {
        "id": "E_yF692HKhD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def occlude(img):\n",
        "  #Your code goes here\n",
        "  return 0\n",
        "\n",
        "#Code below picks a random image, occludes, and shows the occluded image\n",
        "\n",
        "#Modify pick to run around 5 images to get an idea for the effect\n",
        "pick = 10 #Pick any number between 0 and 100 until you get an idea\n",
        "\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "sample_image_occluded = occlude(torch.clone(sample_image))\n",
        "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
        "sample_image_occluded.squeeze(dim=0)\n",
        "\n",
        "\n",
        "ax.title.set_text(\"Metastases\" if sample_label == 1 else \"Benign\")\n",
        "ax.imshow(sample_image_occluded.permute(1,2,0))"
      ],
      "metadata": {
        "id": "VKWO4WjcDhNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results for the non-occluded image\")\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)\n",
        "\n",
        "print(\"Results for the occluded image\")\n",
        "visualize_ig(model, sample_image_occluded, sample_label, sample_image_occluded * 0)"
      ],
      "metadata": {
        "id": "TA0q7cQLK4Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 How can data augmentation help our model?\n",
        "\n",
        "Our model above might be accurate, but it may or may not have done a good job explaining what metastasis looks like. During the training of our model we rescaled the images as a transformation. There are more transformations that we can do. Fill in `custom_transform` to include a set of other transformations. Then run the code to retrain the model with custom transform and see if your model begins to look for different structures using IG. Refer to the following: https://pytorch.org/docs/stable/torchvision/transforms.html for available transformations. Additionally look at `initial_transform` to see how to compose said transformations."
      ],
      "metadata": {
        "id": "87ytbjg1Qi1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_transform(tensor, label):\n",
        "  \"\"\"\n",
        "  Fill in this function with relevant code. \n",
        "  Think of using transformations like Horizontal Flip or Random Perspective\n",
        "  Remember to first convert the image from np array to PILImage and then resize\n",
        "  to 224 by 224 as this is required by most vision models.\n",
        "  After transformations convert from a PIL image to tensor.\n",
        "  Look at the link provided in the directions for more help.\n",
        " \n",
        "  \"\"\" \n",
        "  return 0\n",
        "\n",
        "pcam = PCam(transform=custom_transform)\n",
        "\n",
        "sz = len(pcam)\n",
        "size_train = math.floor(sz - (sz * (1 - PER_TRAIN)))\n",
        "size_test = sz - size_train\n",
        "\n",
        "print(f\"Training size: {size_train}\")\n",
        "print(f\"Testing size: {size_test}\")\n",
        "\n",
        "train_split, test_split = torch.utils.data.random_split(pcam, [size_train, size_test])\n",
        "train_set = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, num_workers=1)\n",
        "test_set = DataLoader(test_split, batch_size=1,\n",
        "                        shuffle=True, num_workers=1)\n"
      ],
      "metadata": {
        "id": "e4yWKCXbQlH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training again, read earlier code about how training works \n",
        "model = train(train_set)"
      ],
      "metadata": {
        "id": "GR4LDK9AR56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving model again but with different name this time\n",
        "model_save_name = 'pcam_model_mod_transforms'\n",
        "path = F\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "AEJQxeSCRFQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading model (code to remount if run into memory issues and have to restart)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'pcam_model_mod_transforms'\n",
        "path = F\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "model = torch.load(path)"
      ],
      "metadata": {
        "id": "A1vjd5PgZbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here you can pick random images, the 0th image is picked right now, to view\n",
        "how IG differs now that you have applied the transform.\n",
        "\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)"
      ],
      "metadata": {
        "id": "j7CaO4ZbWoDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 How does noise effect our model?\n",
        "\n",
        "Fill in the function `noise` to transform an input image by randomly adding noise. Note that the image should almost look identical to the human eye. Run the rest of the code to (1) run the image through the `noise` function and pass it to the classifier, (2) compute its IG, and (3) compare its IG to a non-transformed image. Please note whether or not the performance of your algorithm changes."
      ],
      "metadata": {
        "id": "kV_1Sy7LeHgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(tensor):\n",
        "  #Fill in code here to add noise to an input tensor\n",
        "  return 0\n",
        "\n",
        "\"\"\"\n",
        "After filling in the code above, the code below will show the image\n",
        "with added noise in it. It should look like the normal image but with\n",
        "small colored pixel dots everywhere. \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "sample_image_noise = noise(sample_image)\n",
        "sample_image1 = sample_image_noise.unsqueeze(0).to('cpu')\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
        "sample_image_noise.squeeze(dim=0)\n",
        "\n",
        "ax.title.set_text(\"Metastases\" if sample_label == 1 else \"Benign\")\n",
        "ax.imshow(sample_image_noise.permute(1,2,0))"
      ],
      "metadata": {
        "id": "RgIEfmXTd8Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run the code below to compare model output and IG for model with and without \n",
        "noise added. \n",
        "\"\"\"\n",
        "\n",
        "print(\"Results for the regular image\")\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)\n",
        "\n",
        "print(\"Results for the noise-added image\")\n",
        "visualize_ig(model, sample_image_noise, sample_label, sample_image_noise * 0)"
      ],
      "metadata": {
        "id": "E95XRpFDfPsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "46Rer5qLhHQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}